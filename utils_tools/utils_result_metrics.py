#Importing the necessary
import os
import numpy as np
import math
from tqdm import tqdm
import matplotlib as mpl
from matplotlib import pyplot as plt
from mpl_toolkits.axes_grid1 import make_axes_locatable
import pandas as pd

import torch
import torch.nn as nn
import sklearn
from sklearn.neighbors import NearestNeighbors
from utils_uqmd.utils_uq_vi import VIBPINN
from utils_uqmd.utils_uq_mlp import MLPPINN
from utils_uqmd.utils_uq_cp import CP

from scipy.stats import norm
from utils_uqmd.utils_uq_dropout import DropoutPINN


if torch.backends.mps.is_available():
    device = torch.device("mps")  # Apple Silicon GPU (M1/M2/M3)
elif torch.cuda.is_available():
    device = torch.device("cuda")  # NVIDIA GPU
else:
    device = torch.device("cpu")   # Fallback to CPU


def _to_numpy(x):
    "Torch → NumPy if necessary, otherwise no-op."
    if torch.is_tensor(x):
        return x.detach().cpu().numpy()
    return np.asarray(x)


def _coverage(pred_set, y_true):
    """
    Empirical coverage:  fraction of targets that fall inside the
    predicted interval.

    Parameters
    ----------
    pred_set : array-like, shape (2, N)  OR  (N, 2)
        Row/column order doesn’t matter as long as lower < upper.
    y_true   : array-like, shape (N,)

    Returns
    -------
    float   in the range [0, 1]
    """
    lower, upper = pred_set[0], pred_set[1]
    y_true = y_true.to(lower.device)
    inside = (y_true >= lower) & (y_true <= upper)
    return inside.float().mean().item()


# Define Sharpness
def _sharpness(pred_set):
    """Return the average sharpness of the UQ on the test data set

    Parameters:
        - pred_set: the prediction set generated by the model
    """
    lower = pred_set[0]
    upper = pred_set[1]

    return (upper - lower).mean().item()


def _sdcv(pred_set):
    """ Spatial Dispersion Coefficient of Variation (SDCV) of predicted uncertainty.
    Computes the coefficient of variation (standard deviation / mean) of the predicted interval widths across all samples, which indicates the dispersion of uncertainty estimates.
    
    Parameters
    ----------
    pred_set : array-like, shape (2, N) or (N, 2)
        Predicted interval for each sample (lower and upper bounds).
    
    Returns
    -------
    float 
        The coefficient of variation of interval widths (unitless).
    """
    lower = pred_set[0]
    upper = pred_set[1]
    width = (upper - lower)
    mean_width = width.mean()
    if mean_width.item() == 0:
        return 0.0
    std_width = width.std(unbiased=False)  # population standard deviation
    cv = std_width / mean_width
    return cv.item()


def _interval_score(pred_set, y_true, alpha):
    """ Interval Score (IS) for the prediction intervals.
    A proper scoring rule for interval forecasts that combines interval width and penalties for excluding the true value.
    
    Parameters
    ----------
    pred_set : array-like, shape (2, N) or (N, 2)
        Predicted interval for each sample (lower and upper bounds), corresponding to a (1 - alpha)*100% confidence level.
    y_true : array-like, shape (N,)
        True target values.
    alpha : float, optional
        Significance level of the interval (default 0.05 for 95% confidence).
    
    Returns
    -------
    float 
        The average interval score for the given prediction intervals.
    """
    lower, upper = pred_set[0], pred_set[1]
    y_true = y_true.to(lower.device)
    # Interval components
    width = (upper - lower)
    below_miss = (lower - y_true).clamp(min=0.0)   # (l - y)_+ term
    above_miss = (y_true - upper).clamp(min=0.0)   # (y - u)_+ term
    score = width + (2.0 / alpha) * (below_miss + above_miss)
    return score.mean().item()




# Test coverage for VI model under different level of uncertainty
def vi_test_uncertainties(uqmodel, alphas, X_test, Y_test):
    """
        Evaluate uncertainty metrics (coverage and sharpness) over a range of alphas.

        Parameters:
            uqmodel: callable that returns (pred_set, empirical_coverage)
            alphas: list of uq uncertainty levels (CP:alpha; Drop-Out:drop_out_rate; VI:prior_std)
            X_test, Y_test: test data

        Returns:
            pandas.DataFrame with columns ["alpha", "coverage", "sharpness"]
    """
    if isinstance(uqmodel, VIBPINN):
        results = []

        for alpha in tqdm(alphas):
            alpha_val = float(alpha.item())
            if not (0.0 < alpha_val < 1.0):
                raise ValueError("alpha must be in (0,1) for VI.")
            pred_set = uqmodel.predict(alpha, X_test, n_samples=100)
            coverage = _coverage(pred_set, Y_test)
            sha = _sharpness(pred_set)
            sdcv = _sdcv(pred_set)
            interval_score = _interval_score(pred_set, Y_test, alpha)

            results.append({
                "alpha": alpha_val,
                "coverage": coverage,
                "sharpness": sha,
                "sdcv": sdcv,
                "interval score": interval_score
            })
        return pd.DataFrame(results)

    else:
        raise ValueError("The given model must be VI BPINN")


# Test CP model
def cp_test_uncertainties(uqmodel, alphas, X_test, Y_test, X_cal, Y_cal, X_train, Y_train, heuristic_u, k):
    """
    Test the given cp uq model, using different uq metrics
    """
    # if isinstance(uqmodel, CP):
    results=[]
    for alpha in tqdm(alphas):
        alpha_val = float(alpha)
        if not (0.0 < alpha_val < 1.0):
            raise ValueError("alpha must be in (0,1) for VI.")
        pred_set = uqmodel.predict(alpha, X_test,  X_train,  Y_train, X_cal, Y_cal, heuristic_u=heuristic_u, k=k)
        coverage = _coverage(pred_set, Y_test)
        sha = _sharpness(pred_set)
        sdcv = _sdcv(pred_set)
        interval_score = _interval_score(pred_set, Y_test, alpha)

        results.append({
            "alpha": alpha_val,
            "coverage": coverage,
            "sharpness": sha,
            "sdcv": sdcv,
            "interval score": interval_score
        })
    return pd.DataFrame(results)

    # else:
    #     raise ValueError("The given model must be CP PINN!")

# Test Drop Out model
def do_test_uncertainties(uqmodel, alphas, X_test, Y_test, n_samples):
    """
    Test the given drop-out uq model, using different uq metrics
    """
    if isinstance(uqmodel, DropoutPINN):
        results=[]
        for alpha in tqdm(alphas):
            alpha_val = float(alpha)
            if not (0.0 < alpha_val < 1.0):
                raise ValueError("alpha must be in (0,1) for VI.")
            pred_set = uqmodel.predict(alpha, X_test, n_samples,)
            coverage = _coverage(pred_set, Y_test)
            sha = _sharpness(pred_set)
            sdcv = _sdcv(pred_set)
            interval_score = _interval_score(pred_set, Y_test, alpha)

            results.append({
                "alpha": alpha_val,
                "coverage": coverage,
                "sharpness": sha,
                "sdcv": sdcv,
                "interval score": interval_score
            })
        return pd.DataFrame(results)

    else:
        raise ValueError("The given model must be Dropout PINN!")


# ---------------------------------------------------------------------
#  Test coverage / sharpness for Hamiltonian-MC model
# ---------------------------------------------------------------------
from tqdm import tqdm
import pandas as pd

def hmc_test_uncertainties(uqmodel,
                           alphas,
                           X_test,
                           Y_test,
                           n_samples: int = 1000):
    """
    Evaluate uncertainty metrics (coverage and sharpness) for an HMC-based
    Bayesian PINN across a grid of α values.

    Parameters
    ----------
    uqmodel     : instance of HMCNN
    alphas      : iterable of α values in (0, 1)
    X_test,
    Y_test      : test set tensors
    n_samples   : how many posterior weight draws to use per α
                  (passed to uqmodel.predict)

    Returns
    -------
    pandas.DataFrame with columns ["alpha", "coverage", "sharpness"]
    """

    results = []
    for alpha in tqdm(alphas):
        alpha_val = float(alpha)
        if not (0.0 < alpha_val < 1.0):
            raise ValueError("alpha must be in (0,1) for HMC.")
        # Predict lower/upper bounds
        pred_set = uqmodel.predict(alpha_val, X_test, n_samples=n_samples)
        coverage = _coverage(pred_set, Y_test)
        sha      = _sharpness(pred_set)
        sdcv = _sdcv(pred_set)
        interval_score = _interval_score(pred_set, Y_test, alpha)

        results.append({
            "alpha": alpha_val,
            "coverage": coverage,
            "sharpness": sha,
            "sdcv": sdcv,
            "interval score": interval_score
        })
    return pd.DataFrame(results)


# Test Dist.based md
def dist_test_uncertainties(uqmodel,
                           alphas,
                           X_test,
                           Y_test,
                           heuristic_u: str = "features",
                           n_samples: int = 1000):
    """
    Evaluate uncertainty metrics (coverage and sharpness) for an HMC-based
    Bayesian PINN across a grid of α values.

    Parameters
    ----------
    uqmodel     : instance of HMCNN
    alphas      : iterable of α values in (0, 1)
    X_test,
    Y_test      : test set tensors
    n_samples   : how many posterior weight draws to use per α
                  (passed to uqmodel.predict)

    Returns
    -------
    pandas.DataFrame with columns ["alpha", "coverage", "sharpness"]
    """
    results = []
    for alpha in tqdm(alphas):
        alpha_val = float(alpha)
        if not (0.0 < alpha_val < 1.0):
            raise ValueError("alpha must be in (0,1) for HMC.")
        # Predict lower/upper bounds
        pred_set = uqmodel.predict(alpha_val, X_test, 
                                   heuristic_u=heuristic_u, n_samples=n_samples)
        coverage = _coverage(pred_set, Y_test)
        sha      = _sharpness(pred_set)

        sdcv = _sdcv(pred_set)
        interval_score = _interval_score(pred_set, Y_test, alpha)

        results.append({
            "alpha": alpha_val,
            "coverage": coverage,
            "sharpness": sha,
            "sdcv": sdcv,
            "interval score": interval_score
        })
    return pd.DataFrame(results)



# ------------------------------------------------------------------------------------
def plot_uncertainty(pred_set, x_grid, title='Uncertainty Across Input Grid'):
    """
    Plot the uncertainty (interval width) over the grid.

    Parameters:
    - pred_set: tuple or list containing (lower_bounds, upper_bounds)
    - x_grid: 1D array of x values (same length as pred_set[0] and pred_set[1])
    """
    lower_bounds = np.array(pred_set[0])
    upper_bounds = np.array(pred_set[1])
    uncertainty = (upper_bounds - lower_bounds)

    plt.figure(figsize=(8, 5))
    plt.plot(x_grid, uncertainty, label='Uncertainty Width')
    plt.xlabel('x')
    plt.ylabel('Prediction Interval Width')
    plt.title(title)
    plt.grid(True)
    plt.legend()
    plt.show()
